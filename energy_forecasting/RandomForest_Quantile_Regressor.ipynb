{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e320f9a5",
   "metadata": {},
   "source": [
    "#### Quantile Regression Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4810d77a-d009-45cc-992a-247b4be17ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, date, time\n",
    "import calendar\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "color_pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b787353",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9bf1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model name for result files\n",
    "model_name = \"Random Forest Quantile Regressor\"  \n",
    "\n",
    "# define quantiles \n",
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "\n",
    "# set parametrs for model training and evaluation\n",
    "test_start_date = \"2024-02-22\"  # test/evaluation data start\n",
    "test_size = 168                 # prediction intervall for one split (168 hours = 1 week)\n",
    "n_splits = 52                   # number of splits for TimeSeriesSplit\n",
    "\n",
    "# Set Quantil-training data size in hours which is used for evaluation and final prediction\n",
    "train_windows = {\n",
    "    \"q1_train_window\": 8760 * 7,  # hours\n",
    "    \"q2_train_window\": 8760 * 7,  # hours\n",
    "    \"q3_train_window\": 8760 * 7,  # hours\n",
    "    \"q4_train_window\": 8760 * 7,  # hours\n",
    "    \"q5_train_window\": 8760 * 7,  # hours\n",
    "}\n",
    "\n",
    "# Switch features on or off\n",
    "time_based_features = 2         # 0 for no time based features, 1 for dummy variables, 2 for categorical variables\n",
    "lag_1week = 1                   # 0 for no lag features, 1 for lag features\n",
    "lag_2week_mean = 1              # 0 for no lag features, 1 for lag features\n",
    "lag_4week_mean = 1             # 0 for no lag features, 1 for lag features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8163f",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0deda374",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14768\\3674855404.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/combined_data/combined_energy_data.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Datetime\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Datetime\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# define time zone as ezrope/berlin to account for time shifts in original data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtz_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Europe/Berlin'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1254\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdate_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_date_conversions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdate_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;31m# maybe create a mi on the columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_make_index\u001b[1;34m(self, data, alldata, columns, indexnamerow)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_simple_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_agg_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_complex_date_col\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_processed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_agg_index\u001b[1;34m(self, index, try_parse_dates)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtry_parse_dates\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_parse_dates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m                 \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_date_conv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mna_filter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36mconverter\u001b[1;34m(*date_cols)\u001b[0m\n\u001b[0;32m   1068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1070\u001b[1;33m                 return tools.to_datetime(\n\u001b[0m\u001b[0;32m   1071\u001b[0m                     \u001b[0mensure_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m                     \u001b[0mutc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_convert_and_box_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1076\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1077\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[0mutc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtz\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"utc\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m     result, tz_parsed = objects_to_datetime64ns(\n\u001b[0m\u001b[0;32m    403\u001b[0m         \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\core\\arrays\\datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[1;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[0;32m   2222\u001b[0m     \u001b[0morder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"F\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"F\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"C\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2223\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2224\u001b[1;33m         result, tz_parsed = tslib.array_to_datetime(\n\u001b[0m\u001b[0;32m   2225\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2226\u001b[0m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslibs\\parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\site-packages\\pandas\\_libs\\tslibs\\parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing._parse_dateabbr_string\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime_datetime\u001b[1;34m(cls, data_string, format)\u001b[0m\n\u001b[0;32m    566\u001b[0m     \"\"\"Return a class cls instance based on the input string and the\n\u001b[0;32m    567\u001b[0m     format string.\"\"\"\n\u001b[1;32m--> 568\u001b[1;33m     \u001b[0mtt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff_fraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_strptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m     \u001b[0mtzname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgmtoff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_strptime\u001b[1;34m(data_string, format)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_cache_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mlocale_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_TimeRE_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocale_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         if (_getlang() != locale_time.lang or\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtzname\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlocale_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtzname\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             time.daylight != locale_time.daylight):\n",
      "\u001b[1;32mc:\\Anaconda\\lib\\_strptime.py\u001b[0m in \u001b[0;36m_getlang\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_getlang\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Figure out what the current language is set to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetlocale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocale\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLC_TIME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mLocaleTime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/combined_data/combined_energy_data.csv', parse_dates=[\"Datetime\"], index_col=\"Datetime\")\n",
    "\n",
    "# define time zone as ezrope/berlin to account for time shifts in original data\n",
    "df.index = pd.to_datetime(df.index, utc=True).tz_convert('Europe/Berlin')\n",
    "\n",
    "# convert back to utc to rmeove time shifts\n",
    "df.index = df.index.tz_convert('UTC')\n",
    "\n",
    "# remove tz awareness\n",
    "df.index = df.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad4ca9",
   "metadata": {},
   "source": [
    "#### Prepare and Clean Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b92d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der NaN-Werte in der target-Spalte: 0\n",
      "Indizes der NaN-Werte in Spalte 'A': DatetimeIndex([], dtype='datetime64[ns]', name='Datetime', freq=None)\n",
      "Fehlende Stunden: DatetimeIndex([], dtype='datetime64[ns]', freq='H')\n",
      "Duplikate: Empty DataFrame\n",
      "Columns: [ghi, rain, target, temperature, wind_speed_100m, wind_speed_10m, public_holiday]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = df.loc['2016-01-01':]\n",
    "df.drop_duplicates(inplace=True)\n",
    "nan_count = df['target'].isna().sum()\n",
    "nan_indices = df[df['target'].isna()].index\n",
    "full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "missing_hours = full_range.difference(df.index)\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "print(f\"NaN count in target column: {nan_count}\")\n",
    "print(\"Index\", nan_indices)\n",
    "print(\"Missing hours:\", missing_hours)\n",
    "print(\"Duplicates:\", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3b3cd",
   "metadata": {},
   "source": [
    "#### Create Time Based Features and Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63da0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for time based dummy features\n",
    "if time_based_features == 1:\n",
    "    def create_time_dummies(df):\n",
    "\n",
    "        df['hour'] = df.index.hour.astyep.astype(int)\n",
    "        df['dayofweek'] = df.index.dayofweek.astype(int)\n",
    "        df['month'] = df.index.month.astype(int)\n",
    "        df['year'] = df.index.year.astype(int)\n",
    "        \n",
    "        df_dummies = pd.get_dummies(df, columns=['hour', 'dayofweek', 'month', 'year'], dtype=int, drop_first=True)\n",
    "        \n",
    "        return df_dummies\n",
    "\n",
    "    df = create_time_dummies(df)\n",
    "\n",
    "# function for categorical time based features\n",
    "if time_based_features == 2:\n",
    "    \n",
    "    def create_time_features(df):\n",
    "        df = df.copy()\n",
    "        df['hour'] = df.index.hour.astype(int)\n",
    "        df['dayofweek'] = df.index.dayofweek.astype(int)\n",
    "        df['month'] = df.index.month.astype(int)\n",
    "        df['year'] = df.index.year.astype(int)\n",
    "        df['weekofyear'] = df.index.isocalendar().week.astype(int)\n",
    "    \n",
    "        return df\n",
    "\n",
    "    df = create_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc4c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag 1 week, mean 2 week, mean 4 week\n",
    "if lag_1week == 1:\n",
    "    df['lag_1week'] = df['target'].shift(168)\n",
    "\n",
    "if lag_2week_mean == 1:\n",
    "    df['lag_2week_mean'] = (\n",
    "        df['target'].shift(168) + \n",
    "        df['target'].shift(2*168)\n",
    "    ) / 2\n",
    "\n",
    "if lag_4week_mean == 1:\n",
    "    df['lag_4week_mean'] = (\n",
    "        df['target'].shift(168) + \n",
    "        df['target'].shift(2*168) +\n",
    "        df['target'].shift(3*168) +\n",
    "        df['target'].shift(4*168)\n",
    "    ) / 4\n",
    "\n",
    "# Function to add shifted rolling mean features ---\n",
    "def add_rolling_mean_shifted(df, column, shift_hours, window_hours, name):\n",
    "    \n",
    "    df[name] = df[column].shift(shift_hours).rolling(window=window_hours).mean()\n",
    "    return df\n",
    "\n",
    "# Add Shifted Rolling Means (all shifted 1 week back)\n",
    "rolling_configs = [\n",
    "    (168, 24, 'rolling_1day_shifted'),\n",
    "    (168, 168, 'rolling_1week_shifted'),\n",
    "    (168, 336, 'rolling_2week_shifted'),\n",
    "    (168, 502, 'rolling_3week_shifted'),\n",
    "    (168, 672, 'rolling_4week_shifted'),\n",
    "    (168, 1440, 'rolling_2month_shifted'),\n",
    "    (168, 2160, 'rolling_3month_shifted'),\n",
    "    (168, 8760, 'rolling_1year_shifted')\n",
    "]\n",
    "\n",
    "for shift, window, name in rolling_configs:\n",
    "    df = add_rolling_mean_shifted(df, column='target', shift_hours=shift, window_hours=window, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1109f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features to be scaled\n",
    "columns_to_scale = ['ghi', 'rain', 'temperature', 'wind_speed_100m','wind_speed_10m','lag_1week', 'lag_2week_mean', 'lag_4week_mean',\n",
    "                            'rolling_1day_shifted','rolling_1week_shifted', 'rolling_2week_shifted', 'rolling_3week_shifted', 'rolling_4week_shifted',\n",
    "                              'rolling_2month_shifted', 'rolling_3month_shifted', 'rolling_1year_shifted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebad87",
   "metadata": {},
   "source": [
    "#### Rolling Window and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Copy of dataframe\n",
    "data = df.copy()\n",
    "\n",
    "test_start_date = pd.Timestamp(test_start_date)\n",
    "test_size = test_size\n",
    "n_splits = n_splits\n",
    "\n",
    "train_data = data.loc[:test_start_date]\n",
    "\n",
    "results = pd.DataFrame(index=data.loc[test_start_date:].index)\n",
    "results[\"target\"] = data[\"target\"].reindex(results.index)\n",
    "\n",
    "train_window = train_windows[\"q1_train_window\"]\n",
    "\n",
    "# Cross-validation time series split\n",
    "for split in range(n_splits):\n",
    "    test_start = test_start_date + pd.Timedelta(hours=split * test_size)\n",
    "    test_end = test_start + pd.Timedelta(hours=test_size - 1)\n",
    "    \n",
    "    train_end = test_start - pd.Timedelta(hours=1)\n",
    "    train_start = max(train_end - pd.Timedelta(hours=train_window), data.index[0])\n",
    "\n",
    "    train_data = data.loc[train_start:train_end]\n",
    "    test_data = data.loc[test_start:test_end]\n",
    "\n",
    "    X_train, y_train = train_data.drop(columns=[\"target\"]), train_data[\"target\"]\n",
    "    X_test, y_test = test_data.drop(columns=[\"target\"]), test_data[\"target\"]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train[columns_to_scale] = scaler.fit_transform(X_train[columns_to_scale])\n",
    "    X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "    model = RandomForestQuantileRegressor(n_estimators=729, max_depth=18,min_samples_split=15, min_samples_leaf=5, n_jobs=-1)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    for quantile in quantiles:\n",
    "        y_pred = model.predict(X_test, quantiles=quantile)\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            \"index\": test_data.index,\n",
    "            f\"q{quantile}\": y_pred\n",
    "        }).set_index(\"index\")\n",
    "\n",
    "        results.loc[pred_df.index, f\"q{quantile}\"] = pred_df[f\"q{quantile}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dropna(subset=[col for col in results.columns if col.startswith(\"q\")], inplace=True)\n",
    "\n",
    "# sort quantile columns if quantile crossing occurs\n",
    "def fix_quantile_crossing(results):\n",
    "    \n",
    "    quantile_columns = [col for col in results.columns if col.startswith('q')]\n",
    "    \n",
    "    for idx in results.index:\n",
    "        sorted_values = sorted(results.loc[idx, quantile_columns].values)\n",
    "        results.loc[idx, quantile_columns] = sorted_values\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4d144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe results\n",
    "\n",
    "folder = \"results\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "results.to_csv(f\"{folder}/{model_name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120f682",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss by quantile:\n",
      "Quantile_0.025: 0.32405597956730775\n",
      "Quantile_0.25: 1.3780352993360805\n",
      "Quantile_0.5: 1.6546700148809526\n",
      "Quantile_0.75: 1.2984058636675824\n",
      "Quantile_0.975: 0.24541437127976212\n",
      "\n",
      "Total loss score over all quantiles: 4.900581528731685\n"
     ]
    }
   ],
   "source": [
    "# calucate quantile losses of all predictions\n",
    "quantile_losses = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    \n",
    "    y_pred = results[f'q{q}']\n",
    "    y_true = results['target']\n",
    "    \n",
    "    # pinball loss function multiplied by 2\n",
    "    quantile_loss = np.where(y_pred > y_true, \n",
    "                             2 * (1 - q) * (y_pred - y_true), \n",
    "                             2 * q * (y_true - y_pred))\n",
    "    \n",
    "    quantile_losses[f'Quantile_{q}'] = quantile_loss.mean()\n",
    "\n",
    "# losses of all quantile\n",
    "total_loss_score = sum(quantile_losses.values())\n",
    "\n",
    "# show results\n",
    "print(\"Average loss by quantile:\")\n",
    "for quantile, loss in quantile_losses.items():\n",
    "    print(f\"{quantile}: {loss}\")\n",
    "\n",
    "print(f\"\\nTotal loss score over all quantiles: {total_loss_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93ecee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only relevant target horizons\n",
    "results['hour'] = results.index.hour\n",
    "results['dayofweek'] = results.index.dayofweek\n",
    "\n",
    "horizons_dict = {}\n",
    "\n",
    "# target horizons mapping with dayofweek and hour\n",
    "target_horizons = [\n",
    "    {\"dayofweek\": 4, \"hour\": 12, \"name\": \"36\"},  # Freitag 12:00 Stunde: 36\n",
    "    {\"dayofweek\": 4, \"hour\": 16, \"name\": \"40\"},  # Freitag 16:00 Stunde: 40\n",
    "    {\"dayofweek\": 4, \"hour\": 20, \"name\": \"44\"},  # Freitag 20:00 Stunde: 44\n",
    "    {\"dayofweek\": 5, \"hour\": 12, \"name\": \"60\"},  # Samstag 12:00 Stunde: 60\n",
    "    {\"dayofweek\": 5, \"hour\": 16, \"name\": \"64\"},  # Samstag 16:00 Stunde: 64\n",
    "    {\"dayofweek\": 5, \"hour\": 20, \"name\": \"68\"},  # Samstag 20:00 Stunde: 68\n",
    "]\n",
    "\n",
    "# filter results for target horizons\n",
    "for horizon in target_horizons:\n",
    "    horizon_data = results[(results[\"dayofweek\"] == horizon[\"dayofweek\"]) & (results[\"hour\"] == horizon[\"hour\"])]\n",
    "    horizon_data = horizon_data.drop(columns=[\"hour\", \"dayofweek\"])\n",
    "\n",
    "    horizons_dict[horizon[\"name\"]] = horizon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63cb78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q0.025</th>\n",
       "      <th>q0.25</th>\n",
       "      <th>q0.5</th>\n",
       "      <th>q0.75</th>\n",
       "      <th>q0.975</th>\n",
       "      <th>Total_Loss_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.473065</td>\n",
       "      <td>1.688685</td>\n",
       "      <td>1.857197</td>\n",
       "      <td>1.460740</td>\n",
       "      <td>0.407773</td>\n",
       "      <td>5.887461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.488877</td>\n",
       "      <td>1.465971</td>\n",
       "      <td>1.697615</td>\n",
       "      <td>1.357517</td>\n",
       "      <td>0.401930</td>\n",
       "      <td>5.411910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.306098</td>\n",
       "      <td>1.261639</td>\n",
       "      <td>1.387019</td>\n",
       "      <td>1.088825</td>\n",
       "      <td>0.264168</td>\n",
       "      <td>4.307750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.352144</td>\n",
       "      <td>1.324478</td>\n",
       "      <td>1.872442</td>\n",
       "      <td>1.422901</td>\n",
       "      <td>0.288620</td>\n",
       "      <td>5.260586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.280335</td>\n",
       "      <td>1.257620</td>\n",
       "      <td>1.603817</td>\n",
       "      <td>1.255204</td>\n",
       "      <td>0.238907</td>\n",
       "      <td>4.635884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.247744</td>\n",
       "      <td>1.111901</td>\n",
       "      <td>1.434447</td>\n",
       "      <td>1.117238</td>\n",
       "      <td>0.185403</td>\n",
       "      <td>4.096733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      q0.025     q0.25      q0.5     q0.75    q0.975  Total_Loss_Score\n",
       "36  0.473065  1.688685  1.857197  1.460740  0.407773          5.887461\n",
       "40  0.488877  1.465971  1.697615  1.357517  0.401930          5.411910\n",
       "44  0.306098  1.261639  1.387019  1.088825  0.264168          4.307750\n",
       "60  0.352144  1.324478  1.872442  1.422901  0.288620          5.260586\n",
       "64  0.280335  1.257620  1.603817  1.255204  0.238907          4.635884\n",
       "68  0.247744  1.111901  1.434447  1.117238  0.185403          4.096733"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantile losses target horizons\n",
    "def calculate_quantile_losses(horizons_dict, quantiles):\n",
    "    all_quantile_losses = {}\n",
    "    \n",
    "    for key, df in horizons_dict.items():\n",
    "        quantile_losses = {}\n",
    "        for q in quantiles:\n",
    "            y_pred = df[f'q{q}']\n",
    "            y_true = df['target']\n",
    "            quantile_loss = np.where(y_pred > y_true, 2 * (1 - q) * (y_pred - y_true), 2 * q * (y_true - y_pred))\n",
    "            quantile_losses[f'q{q}'] = quantile_loss.mean()\n",
    "        \n",
    "        total_loss_score = sum(quantile_losses.values())\n",
    "        quantile_losses['Total_Loss_Score'] = total_loss_score\n",
    "        all_quantile_losses[key] = quantile_losses\n",
    "    \n",
    "    return all_quantile_losses\n",
    "\n",
    "quantile_loss_results = calculate_quantile_losses(horizons_dict, quantiles)\n",
    "\n",
    "horizon_results_df = pd.DataFrame(quantile_loss_results).T\n",
    "horizon_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b3f69",
   "metadata": {},
   "source": [
    "#### Final Evaluation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb080b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q0.025               2.148263\n",
       "q0.25                8.110296\n",
       "q0.5                 9.852538\n",
       "q0.75                7.702425\n",
       "q0.975               1.786801\n",
       "Total_Loss_Score    29.600324\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizon_results_df.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
