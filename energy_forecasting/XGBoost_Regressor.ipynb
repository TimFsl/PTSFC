{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e320f9a5",
   "metadata": {},
   "source": [
    "#### XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4810d77a-d009-45cc-992a-247b4be17ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta, date, time\n",
    "import calendar\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "color_pal = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b787353",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9bf1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model name for result files\n",
    "model_name = \"XGBoost\"  \n",
    "\n",
    "# define quantiles \n",
    "quantiles = [0.025, 0.25, 0.5, 0.75, 0.975]\n",
    "\n",
    "# set parametrs for model training and evaluation\n",
    "test_start_date = \"2024-02-22\"  # test/evaluation data start\n",
    "test_size = 168                 # prediction intervall for one split (168 hours = 1 week)\n",
    "n_splits = 52                   # number of splits for TimeSeriesSplit\n",
    "\n",
    "# Set Quantil-training data size in hours which is used for evaluation and final prediction\n",
    "train_windows = {\n",
    "    \"q1_train_window\": 8760 * 7,  # hours\n",
    "    \"q2_train_window\": 8760 * 7,  # hours\n",
    "    \"q3_train_window\": 8760 * 7,  # hours\n",
    "    \"q4_train_window\": 8760 * 7,  # hours\n",
    "    \"q5_train_window\": 8760 * 7,  # hours\n",
    "}\n",
    "\n",
    "# Switch features on or off\n",
    "time_based_features = 2         # 0 for no time based features, 1 for dummy variables, 2 for categorical variables\n",
    "lag_1week = 1                   # 0 for no lag features, 1 for lag features\n",
    "lag_2week_mean = 1              # 0 for no lag features, 1 for lag features\n",
    "lag_4week_mean = 1             # 0 for no lag features, 1 for lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97972fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters for each quantile\n",
    "quantile_params = {\n",
    "0.025: {'colsample_bytree': 0.624208763235312,\n",
    "         'gamma': 0.0145891331311504,\n",
    "         'learning_rate': 0.0474215920097818,\n",
    "         'max_depth': 3,\n",
    "         'min_child_weight': 10,\n",
    "         'n_estimators': 273,\n",
    "         'reg_alpha': 9.260376159150187,\n",
    "         'reg_lambda': 6.670148918073076,\n",
    "         'subsample': 0.624208763235312},\n",
    " 0.25: {'colsample_bytree': 0.8945737421719431,\n",
    "        'gamma': 4.616670111074299,\n",
    "        'learning_rate': 0.0100758033430619,\n",
    "        'max_depth': 4,\n",
    "        'min_child_weight': 5,\n",
    "        'n_estimators': 1559,\n",
    "        'reg_alpha': 1.1010644173403994,\n",
    "        'reg_lambda': 0.4844650974192357,\n",
    "        'subsample': 0.8945737421719431},\n",
    " 0.5: {'colsample_bytree': 0.5021766877363583,\n",
    "       'gamma': 2.469117654739822,\n",
    "       'learning_rate': 0.0121476064807998,\n",
    "       'max_depth': 15,\n",
    "       'min_child_weight': 1,\n",
    "       'n_estimators': 2754,\n",
    "       'reg_alpha': 0.6210633503918601,\n",
    "       'reg_lambda': 7.862171551394882,\n",
    "       'subsample': 0.5021766877363583},\n",
    " 0.75: {'colsample_bytree': 0.839927608049344,\n",
    "        'gamma': 0.8171978525643473,\n",
    "        'learning_rate': 0.0320710915421366,\n",
    "        'max_depth': 5,\n",
    "        'min_child_weight': 4,\n",
    "        'n_estimators': 1937,\n",
    "        'reg_alpha': 0.0107431179093526,\n",
    "        'reg_lambda': 5.208242779679924,\n",
    "        'subsample': 0.839927608049344},\n",
    " 0.975: {'colsample_bytree': 0.6567883304387866,\n",
    "         'gamma': 0.1031657186631934,\n",
    "         'learning_rate': 0.0299069104096565,\n",
    "         'max_depth': 3,\n",
    "         'min_child_weight': 6,\n",
    "         'n_estimators': 921,\n",
    "         'reg_alpha': 8.672234724227451,\n",
    "         'reg_lambda': 6.019190115011864,\n",
    "         'subsample': 0.6567883304387866}\n",
    "         \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8163f",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0deda374",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/combined_data/combined_energy_data.csv', parse_dates=[\"Datetime\"], index_col=\"Datetime\")\n",
    "\n",
    "# define time zone as ezrope/berlin to account for time shifts in original data\n",
    "df.index = pd.to_datetime(df.index, utc=True).tz_convert('Europe/Berlin')\n",
    "\n",
    "# convert back to utc to rmeove time shifts\n",
    "df.index = df.index.tz_convert('UTC')\n",
    "\n",
    "# remove tz awareness\n",
    "df.index = df.index.tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ad4ca9",
   "metadata": {},
   "source": [
    "#### Prepare and Clean Data for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b92d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der NaN-Werte in der target-Spalte: 0\n",
      "Indizes der NaN-Werte in Spalte 'A': DatetimeIndex([], dtype='datetime64[ns]', name='Datetime', freq=None)\n",
      "Fehlende Stunden: DatetimeIndex([], dtype='datetime64[ns]', freq='H')\n",
      "Duplikate: Empty DataFrame\n",
      "Columns: [ghi, rain, target, temperature, wind_speed_100m, wind_speed_10m, public_holiday]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "df = df.loc['2016-01-01':]\n",
    "df.drop_duplicates(inplace=True)\n",
    "nan_count = df['target'].isna().sum()\n",
    "nan_indices = df[df['target'].isna()].index\n",
    "full_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "missing_hours = full_range.difference(df.index)\n",
    "duplicates = df[df.duplicated()]\n",
    "\n",
    "print(f\"NaN count in target column: {nan_count}\")\n",
    "print(\"Index\", nan_indices)\n",
    "print(\"Missing hours:\", missing_hours)\n",
    "print(\"Duplicates:\", duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b3b3cd",
   "metadata": {},
   "source": [
    "#### Create Time Based Features and Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63da0482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for time based dummy features\n",
    "if time_based_features == 1:\n",
    "    def create_time_dummies(df):\n",
    "\n",
    "        df['hour'] = df.index.hour.astyep.astype(int)\n",
    "        df['dayofweek'] = df.index.dayofweek.astype(int)\n",
    "        df['month'] = df.index.month.astype(int)\n",
    "        df['year'] = df.index.year.astype(int)\n",
    "        \n",
    "        df_dummies = pd.get_dummies(df, columns=['hour', 'dayofweek', 'month', 'year'], dtype=int, drop_first=True)\n",
    "        \n",
    "        return df_dummies\n",
    "\n",
    "    df = create_time_dummies(df)\n",
    "\n",
    "# function for categorical time based features\n",
    "if time_based_features == 2:\n",
    "    \n",
    "    def create_time_features(df):\n",
    "        df = df.copy()\n",
    "        df['hour'] = df.index.hour.astype(\"category\")\n",
    "        df['dayofweek'] = df.index.dayofweek.astype(\"category\")\n",
    "        df['month'] = df.index.month.astype(\"category\")\n",
    "        df['year'] = df.index.year.astype(\"category\")\n",
    "        df['weekofyear'] = df.index.isocalendar().week.astype(\"category\")\n",
    "    \n",
    "        return df\n",
    "\n",
    "    df = create_time_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc4c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag 1 week, mean 2 week, mean 4 week\n",
    "if lag_1week == 1:\n",
    "    df['lag_1week'] = df['target'].shift(168)\n",
    "\n",
    "if lag_2week_mean == 1:\n",
    "    df['lag_2week_mean'] = (\n",
    "        df['target'].shift(168) + \n",
    "        df['target'].shift(2*168)\n",
    "    ) / 2\n",
    "\n",
    "if lag_4week_mean == 1:\n",
    "    df['lag_4week_mean'] = (\n",
    "        df['target'].shift(168) + \n",
    "        df['target'].shift(2*168) +\n",
    "        df['target'].shift(3*168) +\n",
    "        df['target'].shift(4*168)\n",
    "    ) / 4\n",
    "\n",
    "# Function to add shifted rolling mean features ---\n",
    "def add_rolling_mean_shifted(df, column, shift_hours, window_hours, name):\n",
    "    \n",
    "    df[name] = df[column].shift(shift_hours).rolling(window=window_hours).mean()\n",
    "    return df\n",
    "\n",
    "# Add Shifted Rolling Means (all shifted 1 week back)\n",
    "rolling_configs = [\n",
    "    (168, 24, 'rolling_1day_shifted'),\n",
    "    (168, 168, 'rolling_1week_shifted'),\n",
    "    (168, 336, 'rolling_2week_shifted'),\n",
    "    (168, 502, 'rolling_3week_shifted'),\n",
    "    (168, 672, 'rolling_4week_shifted'),\n",
    "    (168, 1440, 'rolling_2month_shifted'),\n",
    "    (168, 2160, 'rolling_3month_shifted'),\n",
    "    (168, 8760, 'rolling_1year_shifted')\n",
    "]\n",
    "\n",
    "for shift, window, name in rolling_configs:\n",
    "    df = add_rolling_mean_shifted(df, column='target', shift_hours=shift, window_hours=window, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60924f03",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Feature scaling\n",
    "columns_to_scale = ['ghi', 'rain', 'temperature', 'wind_speed_100m','wind_speed_10m','lag_1week', 'lag_2week_mean', 'lag_4week_mean',\n",
    "                            'rolling_1day_shifted','rolling_1week_shifted', 'rolling_2week_shifted', 'rolling_3week_shifted', 'rolling_4week_shifted',\n",
    "                              'rolling_2month_shifted', 'rolling_3month_shifted', 'rolling_1year_shifted']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ebad87",
   "metadata": {},
   "source": [
    "#### Rolling Window and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0ccf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "test_start_date = pd.Timestamp(test_start_date)\n",
    "test_size = test_size\n",
    "n_splits = n_splits\n",
    "\n",
    "results = pd.DataFrame(index=data.loc[test_start_date:].index)\n",
    "results[\"target\"] = data[\"target\"].reindex(results.index)\n",
    "\n",
    "# Loop quantiles\n",
    "for i, quantile in enumerate(quantiles):\n",
    "    train_window = train_windows[f\"q{i + 1}_train_window\"]\n",
    "    predictions = []\n",
    "    param_set = quantile_params[quantile]\n",
    "\n",
    "    for split in range(n_splits):\n",
    "        test_start = test_start_date + pd.Timedelta(hours=split * test_size)\n",
    "        test_end = test_start + pd.Timedelta(hours=test_size - 1)\n",
    "\n",
    "        train_end = test_start - pd.Timedelta(hours=1)\n",
    "        train_start = max(train_end - pd.Timedelta(hours=train_window), data.index[0])\n",
    "\n",
    "        train_data = data.loc[train_start:train_end]\n",
    "        test_data = data.loc[test_start:test_end]\n",
    "\n",
    "        X_tr = train_data.drop(columns=[\"target\"])\n",
    "        y_tr = train_data[\"target\"]\n",
    "        X_test = test_data.drop(columns=[\"target\"])\n",
    "        y_test = test_data[\"target\"]\n",
    "\n",
    "        time_features = [\"hour\", \"dayofweek\", \"month\", \"year\", \"weekofyear\"]\n",
    "        for col in time_features:\n",
    "            if col in X_tr.columns:\n",
    "                X_tr[col] = X_tr[col].astype(\"category\")\n",
    "                X_test[col] = X_test[col].astype(\"category\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_tr[columns_to_scale] = scaler.fit_transform(X_tr[columns_to_scale])\n",
    "        X_test[columns_to_scale] = scaler.transform(X_test[columns_to_scale])\n",
    "\n",
    "        model = XGBRegressor(\n",
    "            objective=\"reg:quantileerror\",\n",
    "            quantile_alpha=quantile,\n",
    "            learning_rate=param_set[\"learning_rate\"],\n",
    "            max_depth=param_set[\"max_depth\"],\n",
    "            subsample=param_set[\"subsample\"],\n",
    "            colsample_bytree=param_set[\"colsample_bytree\"],\n",
    "            gamma=param_set[\"gamma\"],\n",
    "            min_child_weight=param_set[\"min_child_weight\"],\n",
    "            reg_alpha=param_set[\"reg_alpha\"],\n",
    "            reg_lambda=param_set[\"reg_lambda\"],\n",
    "            n_estimators=param_set[\"n_estimators\"],\n",
    "            tree_method=\"hist\",\n",
    "            enable_categorical=True,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            \"index\": test_data.index,\n",
    "            f\"q{quantile}\": y_pred\n",
    "        }).set_index(\"index\")\n",
    "\n",
    "        results.loc[pred_df.index, f\"q{quantile}\"] = pred_df[f\"q{quantile}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69b9373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.dropna(subset=[col for col in results.columns if col.startswith(\"q\")], inplace=True)\n",
    "\n",
    "# sort quantile columns if quantile crossing occurs\n",
    "def fix_quantile_crossing(results):\n",
    "    \n",
    "    quantile_columns = [col for col in results.columns if col.startswith('q')]\n",
    "    \n",
    "    for idx in results.index:\n",
    "        sorted_values = sorted(results.loc[idx, quantile_columns].values)\n",
    "        results.loc[idx, quantile_columns] = sorted_values\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce4d144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe results\n",
    "\n",
    "folder = \"results\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "results.to_csv(f\"{folder}/{model_name}.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0120f682",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4bc62d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss by quantile:\n",
      "Quantile_0.025: 0.3279046904582244\n",
      "Quantile_0.25: 1.3962762908760882\n",
      "Quantile_0.5: 1.4854316157770682\n",
      "Quantile_0.75: 1.2452596906620066\n",
      "Quantile_0.975: 0.2675266956138962\n",
      "\n",
      "Total loss score over all quantiles: 4.722398983387284\n"
     ]
    }
   ],
   "source": [
    "# calucate quantile losses of all predictions\n",
    "quantile_losses = {}\n",
    "\n",
    "for q in quantiles:\n",
    "    \n",
    "    y_pred = results[f'q{q}']\n",
    "    y_true = results['target']\n",
    "    \n",
    "    # pinball loss function multiplied by 2\n",
    "    quantile_loss = np.where(y_pred > y_true, \n",
    "                             2 * (1 - q) * (y_pred - y_true), \n",
    "                             2 * q * (y_true - y_pred))\n",
    "    \n",
    "    quantile_losses[f'Quantile_{q}'] = quantile_loss.mean()\n",
    "\n",
    "# losses of all quantile\n",
    "total_loss_score = sum(quantile_losses.values())\n",
    "\n",
    "# show results\n",
    "print(\"Average loss by quantile:\")\n",
    "for quantile, loss in quantile_losses.items():\n",
    "    print(f\"{quantile}: {loss}\")\n",
    "\n",
    "print(f\"\\nTotal loss score over all quantiles: {total_loss_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e93ecee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only relevant target horizons\n",
    "results['hour'] = results.index.hour\n",
    "results['dayofweek'] = results.index.dayofweek\n",
    "\n",
    "horizons_dict = {}\n",
    "\n",
    "# target horizons mapping with dayofweek and hour\n",
    "target_horizons = [\n",
    "    {\"dayofweek\": 4, \"hour\": 12, \"name\": \"36\"},  # Freitag 12:00 Stunde: 36\n",
    "    {\"dayofweek\": 4, \"hour\": 16, \"name\": \"40\"},  # Freitag 16:00 Stunde: 40\n",
    "    {\"dayofweek\": 4, \"hour\": 20, \"name\": \"44\"},  # Freitag 20:00 Stunde: 44\n",
    "    {\"dayofweek\": 5, \"hour\": 12, \"name\": \"60\"},  # Samstag 12:00 Stunde: 60\n",
    "    {\"dayofweek\": 5, \"hour\": 16, \"name\": \"64\"},  # Samstag 16:00 Stunde: 64\n",
    "    {\"dayofweek\": 5, \"hour\": 20, \"name\": \"68\"},  # Samstag 20:00 Stunde: 68\n",
    "]\n",
    "\n",
    "# filter results for target horizons\n",
    "for horizon in target_horizons:\n",
    "    horizon_data = results[(results[\"dayofweek\"] == horizon[\"dayofweek\"]) & (results[\"hour\"] == horizon[\"hour\"])]\n",
    "    horizon_data = horizon_data.drop(columns=[\"hour\", \"dayofweek\"])\n",
    "\n",
    "    horizons_dict[horizon[\"name\"]] = horizon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c63cb78a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q0.025</th>\n",
       "      <th>q0.25</th>\n",
       "      <th>q0.5</th>\n",
       "      <th>q0.75</th>\n",
       "      <th>q0.975</th>\n",
       "      <th>Total_Loss_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.541793</td>\n",
       "      <td>1.855171</td>\n",
       "      <td>1.666464</td>\n",
       "      <td>1.325239</td>\n",
       "      <td>0.273044</td>\n",
       "      <td>5.661712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.398925</td>\n",
       "      <td>1.660752</td>\n",
       "      <td>1.511442</td>\n",
       "      <td>1.193345</td>\n",
       "      <td>0.252959</td>\n",
       "      <td>5.017422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.218967</td>\n",
       "      <td>1.078848</td>\n",
       "      <td>1.146590</td>\n",
       "      <td>0.975798</td>\n",
       "      <td>0.308522</td>\n",
       "      <td>3.728725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.292899</td>\n",
       "      <td>1.279497</td>\n",
       "      <td>1.602615</td>\n",
       "      <td>1.400417</td>\n",
       "      <td>0.316587</td>\n",
       "      <td>4.892016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.229682</td>\n",
       "      <td>1.122383</td>\n",
       "      <td>1.318611</td>\n",
       "      <td>1.151482</td>\n",
       "      <td>0.225665</td>\n",
       "      <td>4.047822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.249951</td>\n",
       "      <td>1.062891</td>\n",
       "      <td>1.264852</td>\n",
       "      <td>1.029637</td>\n",
       "      <td>0.171857</td>\n",
       "      <td>3.779188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      q0.025     q0.25      q0.5     q0.75    q0.975  Total_Loss_Score\n",
       "36  0.541793  1.855171  1.666464  1.325239  0.273044          5.661712\n",
       "40  0.398925  1.660752  1.511442  1.193345  0.252959          5.017422\n",
       "44  0.218967  1.078848  1.146590  0.975798  0.308522          3.728725\n",
       "60  0.292899  1.279497  1.602615  1.400417  0.316587          4.892016\n",
       "64  0.229682  1.122383  1.318611  1.151482  0.225665          4.047822\n",
       "68  0.249951  1.062891  1.264852  1.029637  0.171857          3.779188"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quantile losses target horizons\n",
    "def calculate_quantile_losses(horizons_dict, quantiles):\n",
    "    all_quantile_losses = {}\n",
    "    \n",
    "    for key, df in horizons_dict.items():\n",
    "        quantile_losses = {}\n",
    "        for q in quantiles:\n",
    "            y_pred = df[f'q{q}']\n",
    "            y_true = df['target']\n",
    "            quantile_loss = np.where(y_pred > y_true, 2 * (1 - q) * (y_pred - y_true), 2 * q * (y_true - y_pred))\n",
    "            quantile_losses[f'q{q}'] = quantile_loss.mean()\n",
    "        \n",
    "        total_loss_score = sum(quantile_losses.values())\n",
    "        quantile_losses['Total_Loss_Score'] = total_loss_score\n",
    "        all_quantile_losses[key] = quantile_losses\n",
    "    \n",
    "    return all_quantile_losses\n",
    "\n",
    "quantile_loss_results = calculate_quantile_losses(horizons_dict, quantiles)\n",
    "\n",
    "horizon_results_df = pd.DataFrame(quantile_loss_results).T\n",
    "horizon_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426b3f69",
   "metadata": {},
   "source": [
    "#### Final Evaluation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb080b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q0.025               1.932216\n",
       "q0.25                8.059543\n",
       "q0.5                 8.510575\n",
       "q0.75                7.075917\n",
       "q0.975               1.548634\n",
       "Total_Loss_Score    27.126885\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "horizon_results_df.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
